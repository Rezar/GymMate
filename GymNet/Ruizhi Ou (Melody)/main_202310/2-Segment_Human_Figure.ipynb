{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9328d90d",
   "metadata": {},
   "source": [
    "6/29/2023 - Melody\n",
    "\n",
    "**Note** This is from [Google Colab](https://colab.research.google.com/drive/1VyTkcKwy17Bg6Eblc0wtV9GwJo3oEjU6#scrollTo=sFuyni5jWDlV), need to add `!` or `%` in front of some command lines.\n",
    "\n",
    "I had modified it to work an a series of frames, and the output would be the series of frames with only the masked (needed) parts.\n",
    "\n",
    "\n",
    "Object masks from prompts with SAM\n",
    "\n",
    "https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "\n",
    "\n",
    "Segment Anything\n",
    "\n",
    "https://segment-anything.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92654c",
   "metadata": {},
   "source": [
    "### Overall workflow\n",
    "\n",
    "1. Object Detection (YOLOv8)\n",
    "\n",
    "**2. Human figure segmentation (Segment Anything)**\n",
    "\n",
    "3. Pose Estimation (MoveNet)\n",
    "\n",
    "4. Pose Estimation Correction\n",
    "\n",
    "5. Define and classify fitness poses\n",
    "\n",
    "6. Output the text description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d2e8f",
   "metadata": {},
   "source": [
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56edd2",
   "metadata": {},
   "source": [
    "## Environment Set-up\n",
    "\n",
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73365a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc476ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "    !mkdir images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
    "\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bc202",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc71757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis = 0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size = 375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8cee4",
   "metadata": {},
   "source": [
    "### Example image\n",
    "\n",
    "Needs to upload image each time if in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45094d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:,:,i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))\n",
    "\n",
    "def write_masks_to_png(masks: List[Dict[str, Any]], image, path: str) -> None:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    #plt.show()\n",
    "    filename = f\"masks.png\"\n",
    "    plt.savefig(os.path.join(path, filename))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6675c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melody - 6/30/2023\n",
    "# Change all the non-masked (non-human) parts to white\n",
    "\n",
    "def mask_by_white(mask, row, col, img):\n",
    "  # get (i, j) positions of all RGB pixels that are black (i.e. [0, 0, 0])\n",
    "  if mask[row][col] == False:\n",
    "    # set those pixels to white\n",
    "    img[row, col] = [255, 255, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0388ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coordinate of imitating the click\n",
    "star_x = 280\n",
    "star_y = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611f4c8",
   "metadata": {},
   "source": [
    "The order of the files is random. Pls refer to the below link: 7/9/2023 - Melody\n",
    "\n",
    "https://stackoverflow.com/questions/66537490/image-data-is-being-stored-in-different-random-order-in-array-after-reading-from\n",
    "\n",
    "Video to JPG: https://ezgif.com/video-to-jpg\n",
    "\n",
    "Build **images/video_frames** and put the jpgs\n",
    "\n",
    "Build **images/output2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dca05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "path = 'images/video_frames/'\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device = device)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Change directory to the path\n",
    "os.chdir(path)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "sorted_list = sorted(os.listdir())\n",
    "\n",
    "for each_frame in sorted_list:\n",
    "\n",
    "  image = cv2.imread(each_frame) #'ezgif-frame-001.jpg')\n",
    "  print(each_frame)\n",
    "\n",
    "  # Get the row and col number\n",
    "  height, width, channel = image.shape\n",
    "  #height = 1136\n",
    "  #width = 640\n",
    "\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  '''\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(image)\n",
    "  plt.axis('on')\n",
    "  plt.show()\n",
    "  '''\n",
    "  sys.path.append(\"..\")\n",
    "\n",
    "  predictor.set_image(image)\n",
    "\n",
    "  input_point = np.array([[star_x, star_y]])\n",
    "  input_label = np.array([1])\n",
    "\n",
    "  '''\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(image)\n",
    "  show_points(input_point, input_label, plt.gca())\n",
    "  plt.axis('on')\n",
    "  plt.show()\n",
    "  '''\n",
    "  masks, scores, logits = predictor.predict(\n",
    "    point_coords = input_point,\n",
    "    point_labels = input_label,\n",
    "    multimask_output = True,\n",
    "  )\n",
    "\n",
    "  #masks.shape  # (number_of_masks) x H x W\n",
    "\n",
    "  '''\n",
    "  for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize = 18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "  '''\n",
    "\n",
    "  human_mask = masks[2]\n",
    "\n",
    "  for row in range(height):\n",
    "    for col in range(width):\n",
    "      mask_by_white(human_mask, row, col, image)\n",
    "\n",
    "  # Save the third mask\n",
    "  #write_masks_to_png(masks, image, \"segmented\")\n",
    "\n",
    "  # Save image to file\n",
    "\n",
    "  new_image = Image.fromarray(image)\n",
    "  # Create a folder named \"output\" first\n",
    "  output_path = \"/content/images/output2/\" + str(counter) + \".jpeg\"\n",
    "  new_image.save(output_path)\n",
    "\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the folder\n",
    "!zip -r /content/SAM_output2.zip /content/images/output2\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/SAM_output2.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
